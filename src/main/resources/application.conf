pekko {
    loglevel = "INFO"
    stdout-loglevel = "INFO"
    loggers = ["org.apache.pekko.event.slf4j.Slf4jLogger"]
    logging-filter = "org.apache.pekko.event.slf4j.Slf4jLoggingFilter"
    log-dead-letters = 10
    log-dead-letters-during-shutdown = off
    log-dead-letters = off
    actor {
        provider = cluster
        debug {
            receive = off
            lifecycle = off
            unhandled = off
            autoreceive = off
        }
        allow-java-serialization = off
        serializers {
            proto = "org.apache.pekko.remote.serialization.ProtobufSerializer"
            envelope = "org.interscity.htc.core.serializer.EntityEnvelopeSerializer"
            actor-interaction = "org.interscity.htc.core.serializer.ActorInteractionSerializer"
            jackson-json = "org.apache.pekko.serialization.jackson.JacksonJsonSerializer"
            jackson-cbor = "org.apache.pekko.serialization.jackson.JacksonCborSerializer"
        }
        serialization-bindings {
            "org.interscity.htc.core.entity.event.EntityEnvelopeEvent" = envelope
            "scalapb.GeneratedMessage" = proto
            "org.interscity.htc.core.entity.event.ActorInteractionEvent" = actor-interaction
            "org.interscity.htc.core.entity.event.control.execution.TimeManagerRegisterEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.CreateActorsEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.FinishCreationEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.FinishLoadDataEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.LoadDataCreatorRegisterEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.InitializeEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.LoadDataEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.data.BaseEventData" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.report.RegisterReportersEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.report.ReportEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.LoadDataSourceEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.LoadNextEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.ProcessBatchesEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.ProcessNextCreateChunk" = jackson-cbor
            "org.interscity.htc.core.entity.event.SpontaneousEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.FinishEvent" = jackson-cbor
            "org.interscity.htc.core.actor.ActorSerializable" = jackson-cbor
            "org.interscity.htc.core.entity.actor.properties.Properties" = jackson-cbor
            "org.interscity.htc.core.entity.actor.properties.CreatorProperties" = jackson-cbor
            "org.interscity.htc.core.entity.state.BaseState" = jackson-cbor
            "java.time.LocalDateTime" = jackson-cbor
        }
    }

    persistence {
          journal {
            plugin = "pekko.persistence.journal.inmem"
            inmem {
                class = "org.apache.pekko.persistence.journal.inmem.InmemJournal"
            }
          }
          snapshot-store {
            plugin = "pekko.persistence.snapshot-store.local"
            local {
              class = "org.apache.pekko.persistence.snapshot.local.LocalSnapshotStore"
              dir = "/app/hyperbolic-time-chamber/snapshots"
            }
          }
        }

    remote {
        artery {
            canonical {
                hostname = ${clustering.ip}
                port = ${clustering.port}
            }
        }
    }

    cluster {
        downing-provider-class = "org.apache.pekko.cluster.sbr.SplitBrainResolverProvider"
        seed-nodes = [
            "pekko://"${clustering.cluster.name}"@"${clustering.seed-ip}":"${clustering.seed-port}
        ]
        sharding {
            passivation {
                strategy = "default-idle-strategy"
                default-idle-strategy {
                  idle-entity.timeout = 1200.minutes
                }
            }
            verbose-debug-logging = off
            waiting-for-state-timeout = 1 minute
            # Desabilitar snapshots no sharding para melhor performance
            ;remember-entities = false
            ;snapshot-after = 0
        }
    }

    cluster.bootstrap {
        contact-point-discovery {
            service-name  = "pekko-node"
            port-name     = "management"
            protocol      = "http"
        }
    }

    management {
        http {
            hostname = "0.0.0.0"
            port = ${clustering.management-http-port}
        }
    }

    discovery {
        method = config
    }
}

clustering {
    ip = "127.0.0.1"
    ip = ${?CLUSTER_IP}
    port = 1600
    port = ${?CLUSTER_PORT}
    seed-ip = "127.0.0.1"
    seed-ip = ${?CLUSTER_IP}
    seed-ip = ${?SEED_PORT_1600_TCP_ADDR}
    seed-port = 1600
    seed-port = ${?SEED_PORT_1600_TCP_PORT}
    management-http-port = 8558
    management-http-port = ${?MANAGEMENT_HTTP_PORT}
    cluster.name = hyperbolic-time-chamber
}

htc {

    # ‚è±Ô∏è TIME MANAGER CONFIGURATION
        # Otimizado para m√°quina potente: 112 threads + 1TB RAM
        time-manager {
            # Total de inst√¢ncias do pool de TimeManagers no cluster
            # Recomenda√ß√µes para m√°quina potente (112 threads):
            # - 1M viagens: 128
            # - 2-3M viagens: 256
            # - 4-5M viagens: 512
            # - 10M+ viagens: 1024
            # Com 8 nodes: 256 total = 32 por node para 3M viagens
            total-instances = 256
            total-instances = ${?HTC_TIME_MANAGER_INSTANCES}

            # M√°ximo de inst√¢ncias por node
            # 32 inst√¢ncias por node para distribuir melhor a carga
            max-instances-per-node = 32
            max-instances-per-node = ${?HTC_TIME_MANAGER_PER_NODE}

            # üîß BATCH SIZE OTIMIZADO
            # Com 3M trips: batches maiores = melhor throughput
            batch-size = 20000

            # üîß RECOVERY & RESILIENCE CONFIGURATION
            # Snapshot interval: save state every N events
            # Higher values = better performance, lower values = faster recovery
            snapshot-interval = 5000
            snapshot-interval = ${?HTC_TIME_MANAGER_SNAPSHOT_INTERVAL}

            # Actor timeout: how long to wait for actor response (ms)
            # After timeout, actor is removed from runningEvents to avoid deadlock
            # Value is generous because simulation takes time - only prevents infinite hangs
            actor-timeout-ms = 300000
            actor-timeout-ms = ${?HTC_TIME_MANAGER_ACTOR_TIMEOUT_MS}

            # Sync timeout: how long to wait for sync response from Global TM (ms)
            # After recovery, Local TM waits for sync response
            sync-timeout-ms = 60000
            sync-timeout-ms = ${?HTC_TIME_MANAGER_SYNC_TIMEOUT_MS}

            # Stale event max age: events older than this are cleaned (ms)
            # Periodic cleanup removes events that never finished
            stale-event-max-age-ms = 60000
            stale-event-max-age-ms = ${?HTC_TIME_MANAGER_STALE_EVENT_MAX_AGE_MS}

            # Health check interval: periodic health monitoring (seconds)
            # Set to 0 to disable
            health-check-interval-seconds = 60
            health-check-interval-seconds = ${?HTC_TIME_MANAGER_HEALTH_CHECK_INTERVAL}
        }


    report-manager {
        default-strategy = "json"
        enabled-strategies = ["csv", "json"]

        csv {
            prefix = "htc_simulation_"
            directory = "/app/hyperbolic-time-chamber/output/reports/csv"
            number-of-instances = 2
            number-of-instances-per-node = 1
            batch-size = 1000  # Menor para evitar perda de eventos
        }

        json {
            prefix = "htc_simulation_"
            directory = "/app/hyperbolic-time-chamber/output/reports/json"
            number-of-instances = 32
            number-of-instances = ${?HTC_REPORT_JSON_INSTANCES}
            number-of-instances-per-node = 4
            number-of-instances-per-node = ${?HTC_REPORT_JSON_PER_NODE}
            batch-size = 15000
        }
    }

    brokers {
        kafka {
            bootstrap-servers = "localhost:9092"

            consumer {
                group-id-suffix = "htc-group"
                auto-offset-reset = "earliest"
            }
        }
    }

    # üÜï CONFIGURA√á√ÉO DE SIMULA√á√ÉO
    simulation {
        # ID √∫nico da simula√ß√£o (opcional)
        # Se n√£o especificado, ser√° gerado automaticamente
        # Tamb√©m pode ser definido via vari√°vel de ambiente HTC_SIMULATION_ID
        # id = "custom_simulation_001"
        
        # üé≤ SEED PARA REPRODUTIBILIDADE (opcional)
        # Se n√£o especificado, ser√° baseado no timestamp atual
        # Tamb√©m pode ser definido via vari√°vel de ambiente HTC_RANDOM_SEED
        # random-seed = 12345
        # random-seed = ${?HTC_RANDOM_SEED}
    }
}
