pekko {
    loglevel = "INFO"
    stdout-loglevel = "INFO"
    loggers = ["org.apache.pekko.event.slf4j.Slf4jLogger"]
    logging-filter = "org.apache.pekko.event.slf4j.Slf4jLoggingFilter"
    log-dead-letters = 10
    log-dead-letters-during-shutdown = off
    log-dead-letters = off
    actor {
        provider = cluster
        
        # üîß DISPATCHER CONFIGURATION
        # Default dispatcher otimizado para simula√ß√µes CPU-intensivas
        # M√°quina: 2x AMD EPYC 7453 (112 threads, 1TB RAM)
        default-dispatcher {
            type = Dispatcher
            executor = "fork-join-executor"
            
            fork-join-executor {
                # Paralelismo baseado no n√∫mero de CPUs
                # Com 112 threads: usamos fator 0.85 para melhor distribui√ß√£o
                # Total: ~96 threads para simula√ß√£o (deixa mais espa√ßo para sistema)
                parallelism-factor = 0.85
                
                # M√≠nimo de threads no pool (deixar n√∫cleos f√≠sicos)
                parallelism-min = 56
                
                # M√°ximo de threads no pool
                # Reduzido para 96 para evitar conten√ß√£o
                parallelism-max = 96
                
                # Tamanho da task queue
                task-peeking-mode = FIFO
            }
            
            # Throughput: n√∫mero de mensagens processadas antes de trocar para outro ator
            # Aumentado para melhor throughput em m√°quina potente
            throughput = 20
        }
        
        # Dispatcher interno do Pekko (heartbeats, etc)
        # Separado para evitar bloqueio por simula√ß√µes pesadas
        # REFOR√áADO para evitar deadlocks
        internal-dispatcher {
            type = Dispatcher
            executor = "fork-join-executor"
            
            fork-join-executor {
                parallelism-factor = 1.0
                parallelism-min = 12
                parallelism-max = 16
            }
            
            throughput = 10
        }
        
        debug {
            receive = off
            lifecycle = off
            unhandled = off
            autoreceive = off
        }
        allow-java-serialization = off
        serializers {
            proto = "org.apache.pekko.remote.serialization.ProtobufSerializer"
            envelope = "org.interscity.htc.core.serializer.EntityEnvelopeSerializer"
            actor-interaction = "org.interscity.htc.core.serializer.ActorInteractionSerializer"
            jackson-json = "org.apache.pekko.serialization.jackson.JacksonJsonSerializer"
            jackson-cbor = "org.apache.pekko.serialization.jackson.JacksonCborSerializer"
        }
        serialization-bindings {
            "org.interscity.htc.core.entity.event.EntityEnvelopeEvent" = envelope
            "scalapb.GeneratedMessage" = proto
            "org.interscity.htc.core.entity.event.ActorInteractionEvent" = actor-interaction
            "org.interscity.htc.core.entity.event.control.execution.TimeManagerRegisterEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.CreateActorsEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.FinishCreationEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.FinishLoadDataEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.LoadDataCreatorRegisterEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.InitializeEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.LoadDataEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.data.BaseEventData" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.report.RegisterReportersEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.report.ReportEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.LoadDataSourceEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.LoadNextEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.ProcessBatchesEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.control.load.ProcessNextCreateChunk" = jackson-cbor
            "org.interscity.htc.core.entity.event.SpontaneousEvent" = jackson-cbor
            "org.interscity.htc.core.entity.event.FinishEvent" = jackson-cbor
            "org.interscity.htc.core.actor.ActorSerializable" = jackson-cbor
            "org.interscity.htc.core.entity.actor.properties.Properties" = jackson-cbor
            "org.interscity.htc.core.entity.actor.properties.CreatorProperties" = jackson-cbor
            "org.interscity.htc.core.entity.state.BaseState" = jackson-cbor
            "java.time.LocalDateTime" = jackson-cbor
        }
    }

    # Persistence usando in-memory (sem Cassandra)
    persistence {
      journal {
        plugin = "pekko.persistence.journal.inmem"
        auto-start-journals = ["pekko.persistence.journal.inmem"]
      }
      snapshot-store {
        plugin = "pekko.persistence.snapshot-store.local"
        auto-start-snapshot-stores = ["pekko.persistence.snapshot-store.local"]
      }
    }

    # Configura√ß√£o do Local Snapshot Store
    persistence.snapshot-store.local {
      dir = "/app/hyperbolic-time-chamber/snapshots"
    }

    remote {
        artery {
            canonical {
                hostname = ${clustering.ip}
                port = ${clustering.port}
            }
            
            # üîß ARTERY TRANSPORT TUNING
            # Configura√ß√£o para alto throughput de mensagens
            
            # N√∫mero de threads para enviar mensagens
            advanced {
                # Tamanho m√°ximo de mensagem (padr√£o: 256 KiB)
                maximum-frame-size = 2 MiB
                
                # Buffer para mensagens de sa√≠da
                # Com 1TB RAM: buffer MUITO maior para evitar backpressure
                outbound-message-queue-size = 65536
                
                # Timeout para estabelecer conex√£o
                handshake-timeout = 60 s
                
                # Timeout para injetar falhas (MUITO aumentado para alta carga)
                inject-failure-timeout = 60 s
                
                # Configura√ß√£o de compress√£o (desabilitada para performance)
                compression {
                    actor-refs.max = 256
                    manifests.max = 256
                }
                
                # Flight recorder (desabilitado para performance)
                flight-recorder {
                    enabled = off
                }
            }
            
            # üîß LARGE MESSAGE STREAM TUNING
            # Para mensagens grandes (batches, relat√≥rios)
            large-message-destinations = [
                "/system/sharding/*",
                "/user/report-manager-*",
                "/user/load-manager"
            ]
        }
        
        # üîß WATCH FAILURE DETECTOR - ULTRA TOLERANTE
        # Detector de falhas para watch/unwatch de atores
        watch-failure-detector {
            # Threshold MUITO alto para evitar falsos positivos
            threshold = 16.0
            
            # Intervalo de heartbeat mais lento
            heartbeat-interval = 3 s
            
            # Pausa MUITO aceit√°vel entre heartbeats
            acceptable-heartbeat-pause = 15 s
            
            # Intervalo maior para verificar falhas
            unreachable-nodes-reaper-interval = 5 s
            
            # Tempo de resposta esperado maior
            expected-response-after = 5 s
        }
    }

    cluster {
        downing-provider-class = "org.apache.pekko.cluster.sbr.SplitBrainResolverProvider"
        seed-nodes = [
            "pekko://"${clustering.cluster.name}"@"${clustering.seed-ip}":"${clustering.seed-port}
        ]
        
        # üîß HEARTBEAT & FAILURE DETECTION TUNING - ULTRA TOLERANTE
        # Ajustado para simula√ß√µes de 3M+ viagens com poss√≠veis GC pauses longos
        # Valores MUITO conservadores para evitar deadlocks
        
        failure-detector {
            # Threshold MUITO alto para considerar node como falho (padr√£o: 8.0)
            # Com 3M trips: poss√≠veis GC pauses longos - threshold 16.0
            threshold = 16.0
            
            # N√∫mero m√≠nimo de amostras antes de come√ßar a detectar falhas
            min-std-deviation = 200 ms
            
            # Intervalo aceit√°vel entre heartbeats MUITO maior
            acceptable-heartbeat-pause = 10 s
            
            # Intervalo m√°ximo esperado entre heartbeats
            expected-response-after = 5 s
        }
        
        # Configura√ß√£o do heartbeat do cluster
        # Intervalo de envio de heartbeats mais lento
        heartbeat-interval = 2000 ms
        
        # N√∫mero de heartbeats perdidos antes de marcar como unreachable
        # MUITO maior para tolerar GC pauses
        acceptable-heartbeat-pause = 12 s
        
        # Intervalo maior para detectar nodes unreachable
        unreachable-nodes-reaper-interval = 8 s
        
        # Intervalo para publicar estat√≠sticas do cluster
        publish-stats-interval = 10 s
        
        # üîß SPLIT BRAIN RESOLVER TUNING - ULTRA CONSERVADOR
        # Configura√ß√£o para resolver parti√ß√µes de rede
        split-brain-resolver {
            # Estrat√©gia: keep-majority (mant√©m maioria dos nodes)
            active-strategy = keep-majority
            
            # Tempo de estabilidade MUITO maior antes de tomar decis√£o
            # Aguarda 40s para ter certeza absoluta
            stable-after = 40s
            
            # Configura√ß√£o espec√≠fica para keep-majority
            keep-majority {
                # Papel (role) a considerar na decis√£o (opcional)
                # role = ""
            }
            
            # N√ÉO derrubar todos quando inst√°vel (mais conservador)
            down-all-when-unstable = off
        }
        
        sharding {
            passivation {
                strategy = "default-idle-strategy"
                default-idle-strategy {
                  idle-entity.timeout = 1200.minutes
                }
            }
            verbose-debug-logging = off
            waiting-for-state-timeout = 1 minute
            # Desabilitar snapshots no sharding para melhor performance
            remember-entities = false
            snapshot-after = 0
            
            # üîß Timeout para esperar coordenador (MUITO aumentado)
            coordinator-failure-backoff = 10 s
            retry-interval = 5 s
            
            # Buffer de mensagens pendentes
            # Com 1TB RAM: buffer ENORME para evitar backpressure
            buffer-size = 1000000
            
            # Timeout para handoff de shards
            handoff-timeout = 180 s
            
            # Timeout para atualiza√ß√£o de state
            updating-state-timeout = 10 s
            
            # Backoff para entity restart
            entity-restart-backoff = 30 s
        }
    }

    cluster.bootstrap {
        contact-point-discovery {
            service-name  = "pekko-node"
            port-name     = "management"
            protocol      = "http"
        }
    }

    management {
        http {
            hostname = "0.0.0.0"
            port = ${clustering.management-http-port}
        }
    }

    discovery {
        method = config
    }
}

clustering {
    ip = "127.0.0.1"
    ip = ${?CLUSTER_IP}
    port = 1600
    port = ${?CLUSTER_PORT}
    seed-ip = "127.0.0.1"
    seed-ip = ${?CLUSTER_IP}
    seed-ip = ${?SEED_PORT_1600_TCP_ADDR}
    seed-port = 1600
    seed-port = ${?SEED_PORT_1600_TCP_PORT}
    management-http-port = 8558
    management-http-port = ${?MANAGEMENT_HTTP_PORT}
    cluster.name = hyperbolic-time-chamber
}

htc {

    # ‚è±Ô∏è TIME MANAGER CONFIGURATION
    # Otimizado para m√°quina potente: 112 threads + 1TB RAM
    time-manager {
        # Total de inst√¢ncias do pool de TimeManagers no cluster
        # Recomenda√ß√µes para m√°quina potente (112 threads):
        # - 1M viagens: 128 
        # - 2-3M viagens: 256
        # - 4-5M viagens: 512
        # - 10M+ viagens: 1024
        # Com 8 nodes: 256 total = 32 por node para 3M viagens
        total-instances = 256
        total-instances = ${?HTC_TIME_MANAGER_INSTANCES}
        
        # M√°ximo de inst√¢ncias por node
        # 32 inst√¢ncias por node para distribuir melhor a carga
        max-instances-per-node = 32
        max-instances-per-node = ${?HTC_TIME_MANAGER_PER_NODE}
        
        # üîß BATCH SIZE OTIMIZADO
        # Com 3M trips: batches maiores = melhor throughput
        batch-size = 20000
        
        # üîß RECOVERY & RESILIENCE CONFIGURATION
        # Snapshot interval: save state every N events
        # Higher values = better performance, lower values = faster recovery
        snapshot-interval = 5000
        snapshot-interval = ${?HTC_TIME_MANAGER_SNAPSHOT_INTERVAL}
        
        # Actor timeout: how long to wait for actor response (ms)
        # After timeout, actor is removed from runningEvents to avoid deadlock
        # Value is generous because simulation takes time - only prevents infinite hangs
        actor-timeout-ms = 300000
        actor-timeout-ms = ${?HTC_TIME_MANAGER_ACTOR_TIMEOUT_MS}
        
        # Sync timeout: how long to wait for sync response from Global TM (ms)
        # After recovery, Local TM waits for sync response
        sync-timeout-ms = 60000
        sync-timeout-ms = ${?HTC_TIME_MANAGER_SYNC_TIMEOUT_MS}
        
        # Stale event max age: events older than this are cleaned (ms)
        # Periodic cleanup removes events that never finished
        stale-event-max-age-ms = 60000
        stale-event-max-age-ms = ${?HTC_TIME_MANAGER_STALE_EVENT_MAX_AGE_MS}
        
        # Health check interval: periodic health monitoring (seconds)
        # Set to 0 to disable
        health-check-interval-seconds = 60
        health-check-interval-seconds = ${?HTC_TIME_MANAGER_HEALTH_CHECK_INTERVAL}
    }

    report-manager {
        default-strategy = "json"
        enabled-strategies = ["csv", "json"]

        csv {
            prefix = "htc_simulation_"
            directory = "/app/hyperbolic-time-chamber/output/reports/csv"
            # Otimizado para m√°quina potente (112 threads + 1TB RAM)
            # Recomenda√ß√µes:
            # - 1M viagens: 8 total, 1 per node
            # - 2-3M viagens: 32 total, 4 per node (OTIMIZADO)
            # - 4-5M viagens: 64 total, 8 per node
            # - 10M+ viagens: 128 total, 16 per node
            number-of-instances = 32
            number-of-instances = ${?HTC_REPORT_CSV_INSTANCES}
            number-of-instances-per-node = 4
            number-of-instances-per-node = ${?HTC_REPORT_CSV_PER_NODE}
            # Com mais mem√≥ria: batches maiores = melhor performance
            batch-size = 15000
        }

        json {
            prefix = "htc_simulation_"
            directory = "/app/hyperbolic-time-chamber/output/reports/json"
            # Otimizado para m√°quina potente
            number-of-instances = 32
            number-of-instances = ${?HTC_REPORT_JSON_INSTANCES}
            number-of-instances-per-node = 4
            number-of-instances-per-node = ${?HTC_REPORT_JSON_PER_NODE}
            # Com mais mem√≥ria: batches maiores = melhor performance
            batch-size = 15000
        }
    }

    # Databases removidas (Cassandra n√£o mais usado)

    brokers {
        kafka {
            bootstrap-servers = "localhost:9092"

            consumer {
                group-id-suffix = "htc-group"
                auto-offset-reset = "earliest"
            }
        }
    }

    # üÜï CONFIGURA√á√ÉO DE SIMULA√á√ÉO
    simulation {
        # ID √∫nico da simula√ß√£o (opcional)
        # Se n√£o especificado, ser√° gerado automaticamente
        # Tamb√©m pode ser definido via vari√°vel de ambiente HTC_SIMULATION_ID
        # id = "custom_simulation_001"
        
        # üé≤ SEED PARA REPRODUTIBILIDADE (opcional)
        # Se n√£o especificado, ser√° baseado no timestamp atual
        # Tamb√©m pode ser definido via vari√°vel de ambiente HTC_RANDOM_SEED
        # random-seed = 12345
        # random-seed = ${?HTC_RANDOM_SEED}
    }
}