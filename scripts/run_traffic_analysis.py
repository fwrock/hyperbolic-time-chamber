#!/usr/bin/env python3
"""
Script principal para executar anÃ¡lise de trÃ¡fego
Com gerenciamento correto de imports
"""

import sys
import os
from pathlib import Path
import pandas as pd

# Adiciona o diretÃ³rio scripts ao PYTHONPATH
SCRIPT_DIR = Path(__file__).parent.resolve()
sys.path.insert(0, str(SCRIPT_DIR))

import argparse
import logging
from datetime import datetime

# Agora podemos fazer imports absolutos
from config import CASSANDRA_CONFIG, OUTPUT_PATH, LOGGING_CONFIG
from data_sources.cassandra_source import CassandraDataSource
from data_sources.file_sources import CSVDataSource, DataSourceFactory
from analysis.traffic_analyzer import TrafficAnalyzer
from visualization.traffic_viz import TrafficVisualizer
from reports.report_generator import TrafficReportGenerator

def setup_logging():
    """Configura o sistema de logging"""
    logging.basicConfig(**LOGGING_CONFIG)
    return logging.getLogger(__name__)

def run_cassandra_analysis(limit: int = None):
    """Executa anÃ¡lise usando dados do Cassandra"""
    logger = logging.getLogger(__name__)
    
    try:
        # Conectar ao Cassandra
        logger.info("ğŸ”Œ Conectando ao Cassandra...")
        cassandra_source = CassandraDataSource()
        
        # Carregar dados
        logger.info(f"ğŸ“Š Carregando dados (limite: {limit})...")
        data = cassandra_source.get_vehicle_flow_data(limit=limit)
        
        if data.empty:
            logger.warning("âš ï¸ Nenhum dado encontrado no Cassandra")
            return
        
        logger.info(f"âœ… Carregados {len(data)} registros")
        
        # Executar anÃ¡lise
        run_analysis(data, "cassandra", logger)
        
    except Exception as e:
        logger.error(f"âŒ Erro na anÃ¡lise do Cassandra: {e}")
        raise

def run_csv_analysis(file_path: str):
    """Executa anÃ¡lise usando dados de arquivo CSV"""
    logger = logging.getLogger(__name__)
    
    try:
        # Carregar dados do CSV
        logger.info(f"ğŸ“ Carregando dados do CSV: {file_path}")
        
        # Converter caminho relativo para absoluto se necessÃ¡rio
        if not os.path.isabs(file_path):
            file_path = os.path.join(SCRIPT_DIR.parent, file_path)
        
        # Se for um arquivo especÃ­fico, carregar diretamente
        if os.path.isfile(file_path):
            data = pd.read_csv(file_path)
            logger.info(f"âœ… Carregados {len(data)} registros do arquivo CSV")
        else:
            # Se for um diretÃ³rio, usar o mÃ©todo original
            csv_source = CSVDataSource(file_path)
            data = csv_source.load_vehicle_flow_data()
        
        if data.empty:
            logger.warning("âš ï¸ Nenhum dado encontrado no arquivo CSV")
            return
        
        logger.info(f"âœ… Carregados {len(data)} registros do CSV")
        
        # Executar anÃ¡lise
        run_analysis(data, "csv", logger)
        
    except Exception as e:
        logger.error(f"âŒ Erro na anÃ¡lise do CSV: {e}")
        raise

def run_analysis(data, source_type: str, logger):
    """Executa a anÃ¡lise completa dos dados"""
    
    try:
        # AnÃ¡lise dos dados
        logger.info("ğŸ” Iniciando anÃ¡lise de trÃ¡fego...")
        analyzer = TrafficAnalyzer()
        
        # Executar anÃ¡lises
        analysis_results = analyzer.analyze_traffic_flow(data)
        
        # VisualizaÃ§Ãµes
        logger.info("ğŸ“Š Gerando visualizaÃ§Ãµes...")
        visualizer = TrafficVisualizer(data)
        
        # Criar visualizaÃ§Ãµes padrÃ£o
        timeline_fig = visualizer.create_traffic_flow_timeline()
        distribution_fig = visualizer.create_vehicle_distribution()
        patterns_fig = visualizer.create_mobility_patterns()
        
        # Gerar relatÃ³rios
        logger.info("ğŸ“ Gerando relatÃ³rios...")
        report_generator = TrafficReportGenerator()
        
        # RelatÃ³rio JSON detalhado
        json_report_path = report_generator.generate_detailed_report(
            analysis_results, 
            OUTPUT_PATH / "traffic_analysis_report.json"
        )
        
        # RelatÃ³rio Markdown
        md_report_path = report_generator.generate_summary_report(
            analysis_results,
            OUTPUT_PATH / "traffic_summary.md"
        )
        
        # Dashboard HTML
        dashboard_path = report_generator.generate_html_dashboard(
            analysis_results,
            [timeline_fig, distribution_fig, patterns_fig],
            OUTPUT_PATH / "traffic_dashboard.html"
        )
        
        # ğŸ†• GERAR PDF ACADÃŠMICO
        logger.info("ğŸ“„ Gerando PDF acadÃªmico para artigo...")
        try:
            from visualization.academic_viz import create_academic_pdf_report
            
            academic_pdfs = create_academic_pdf_report(
                'traffic',
                data=data,
                analysis_results=analysis_results,
                output_path=OUTPUT_PATH / "academic_reports",
                filename="traffic_analysis_academic.pdf"
            )
            
            for pdf_path in academic_pdfs:
                logger.info(f"ğŸ“„ PDF acadÃªmico gerado: {pdf_path}")
                
        except ImportError as e:
            logger.warning(f"âš ï¸ DependÃªncias para PDF nÃ£o encontradas: {e}")
            logger.info("ğŸ’¡ Para gerar PDFs, instale: pip install matplotlib seaborn plotly kaleido")
        except Exception as e:
            logger.warning(f"âš ï¸ Erro ao gerar PDF acadÃªmico: {e}")
        
        # Log dos arquivos gerados
        logger.info("âœ… AnÃ¡lise concluÃ­da! Arquivos gerados:")
        logger.info(f"  ğŸ“‹ RelatÃ³rio detalhado: {json_report_path}")
        logger.info(f"  ğŸ“ Resumo: {md_report_path}")
        logger.info(f"  ğŸ“Š Dashboard: {dashboard_path}")
        
    except Exception as e:
        logger.error(f"âŒ Erro durante anÃ¡lise: {e}")
        raise
        
        # AnÃ¡lises bÃ¡sicas
        summary = analyzer.basic_statistics(data)
        logger.info(f"ğŸ“ˆ EstatÃ­sticas bÃ¡sicas calculadas")
        
        # AnÃ¡lise temporal
        temporal = analyzer.temporal_analysis(data)
        logger.info(f"â° AnÃ¡lise temporal concluÃ­da")
        
        # AnÃ¡lise espacial (se houver dados de localizaÃ§Ã£o)
        spatial = None
        if 'latitude' in data.columns and 'longitude' in data.columns:
            spatial = analyzer.spatial_analysis(data)
            logger.info(f"ğŸ—ºï¸ AnÃ¡lise espacial concluÃ­da")
        
        # VisualizaÃ§Ãµes
        logger.info("ğŸ“Š Gerando visualizaÃ§Ãµes...")
        visualizer = TrafficVisualizer(data)
        
        # GrÃ¡ficos temporais
        logger.info("ğŸ“ˆ Criando grÃ¡ficos temporais...")
        
        # GrÃ¡ficos de estatÃ­sticas  
        logger.info("ğŸ“Š Criando grÃ¡ficos de distribuiÃ§Ã£o...")
        
        # Mapa (se dados espaciais disponÃ­veis)
        if spatial and 'latitude' in data.columns:
            logger.info("ğŸ—ºï¸ Criando mapa de trÃ¡fego...")
        
        logger.info("âœ… VisualizaÃ§Ãµes salvas em: " + str(OUTPUT_PATH))
        
        # Gerar relatÃ³rio
        logger.info("ğŸ“‹ Gerando relatÃ³rio...")
        
        report_data = {
            'summary': summary,
            'temporal': temporal,
            'spatial': spatial,
            'source_type': source_type,
            'timestamp': datetime.now().isoformat(),
            'total_records': len(data)
        }
        
        # Salvar dados do relatÃ³rio em JSON
        report_file = OUTPUT_PATH / "reports" / f"traffic_analysis_report_{source_type}.json"
        report_file.parent.mkdir(parents=True, exist_ok=True)
        
        # FunÃ§Ã£o para converter dados para JSON serializable
        def json_serializer(obj):
            if hasattr(obj, 'isoformat'):
                return obj.isoformat()
            elif hasattr(obj, 'item'):
                return obj.item()
            elif hasattr(obj, 'tolist'):
                return obj.tolist()
            return str(obj)
        
        with open(report_file, 'w') as f:
            import json
            json.dump(report_data, f, indent=2, default=json_serializer)
        
        logger.info("âœ… RelatÃ³rio gerado com sucesso!")
        
        # EstatÃ­sticas finais
        logger.info("="*60)
        logger.info("ğŸ¯ ANÃLISE CONCLUÃDA COM SUCESSO!")
        logger.info(f"ğŸ“Š Total de registros: {len(data)}")
        logger.info(f"ğŸ“ˆ Fonte dos dados: {source_type.upper()}")
        logger.info(f"ğŸ“ Outputs salvos em: {OUTPUT_PATH}")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"âŒ Erro durante anÃ¡lise: {e}")
        raise

def main():
    """FunÃ§Ã£o principal"""
    parser = argparse.ArgumentParser(description="Traffic Analysis System")
    parser.add_argument('source', choices=['cassandra', 'csv'], help='Fonte dos dados')
    parser.add_argument('--limit', type=int, help='Limite de registros (Cassandra)')
    parser.add_argument('--file', type=str, help='Arquivo CSV (para source=csv)')
    
    args = parser.parse_args()
    
    # Setup logging
    logger = setup_logging()
    logger.info("ğŸš€ Iniciando Traffic Analysis System...")
    
    # Garantir que diretÃ³rios existam
    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
    
    try:
        if args.source == 'cassandra':
            run_cassandra_analysis(args.limit)
        elif args.source == 'csv':
            if not args.file:
                logger.error("âŒ --file Ã© obrigatÃ³rio para source=csv")
                return 1
            run_csv_analysis(args.file)
            
        return 0
        
    except Exception as e:
        logger.error(f"ğŸ’¥ Falha na execuÃ§Ã£o: {e}")
        return 1

if __name__ == "__main__":
    exit(main())